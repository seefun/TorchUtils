# https://github.com/yurayli/fisheries-monitoring/blob/e82688464f0f0a5a038589ccb6957df9a764c697/fish_ssd/one_cycle_lr.py

import numpy as np


class OneCycleScheduler(object):

    def __init__(self, optimizer, epochs, train_loader, accumulation_steps=1, max_lr=3e-3,
                 moms=(.95, .85), div_factor=25, sep_ratio=0.3, final_div=None):

        self.optimizer = optimizer

        if isinstance(max_lr, list) or isinstance(max_lr, tuple):
            if len(max_lr) != len(optimizer.param_groups):
                raise ValueError("expected {} max_lr, got {}".format(
                    len(optimizer.param_groups), len(max_lr)))
            self.max_lrs = list(max_lr)
            self.init_lrs = [lr / div_factor for lr in self.max_lrs]
        else:
            self.max_lrs = [max_lr] * len(optimizer.param_groups)
            self.init_lrs = [max_lr / div_factor] * len(optimizer.param_groups)

        if final_div is None:
            final_div = div_factor * 1e4
        self.final_lrs = [lr / final_div for lr in self.max_lrs]
        self.moms = moms

        total_iteration = epochs * len(train_loader) // accumulation_steps
        self.up_iteration = int(total_iteration * sep_ratio)
        self.down_iteration = total_iteration - self.up_iteration

        self.curr_iter = 0
        self._assign_lr_mom(self.init_lrs, [moms[0]] * len(optimizer.param_groups))

    def _assign_lr_mom(self, lrs, moms):
        for param_group, lr, mom in zip(self.optimizer.param_groups, lrs, moms):
            param_group['lr'] = lr
            param_group['betas'] = (mom, 0.999)

    def _annealing_cos(self, start, end, pct):
        cos_out = np.cos(np.pi * pct) + 1
        return end + (start - end) / 2 * cos_out

    def step(self):
        self.curr_iter += 1

        if self.curr_iter <= self.up_iteration:
            pct = self.curr_iter / self.up_iteration
            curr_lrs = [self._annealing_cos(min_lr, max_lr, pct)
                        for min_lr, max_lr in zip(self.init_lrs, self.max_lrs)]
            curr_moms = [self._annealing_cos(self.moms[0], self.moms[1], pct)
                         for _ in range(len(self.optimizer.param_groups))]
        else:
            pct = (self.curr_iter - self.up_iteration) / self.down_iteration
            curr_lrs = [self._annealing_cos(max_lr, final_lr, pct)
                        for max_lr, final_lr in zip(self.max_lrs, self.final_lrs)]
            curr_moms = [self._annealing_cos(self.moms[1], self.moms[0], pct)
                         for _ in range(len(self.optimizer.param_groups))]

        self._assign_lr_mom(curr_lrs, curr_moms)

# the official implementation:
# torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps=None, epochs=None, 
#                                     steps_per_epoch=None, pct_start=0.3, anneal_strategy='cos', 
#                                     cycle_momentum=True, base_momentum=0.85, max_momentum=0.95, 
#                                     div_factor=25.0, final_div_factor=10000.0, last_epoch=-1)
